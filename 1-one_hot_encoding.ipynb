{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding\n",
    "In the beginning were the words. So very many words. Our first step is to convert all the words to numbers so we can do math on them.\n",
    "\n",
    "Imagine that our goal is to create the computer that responds to our voice commands. Itâ€™s our job to build the transformer that converts (or transduces) a sequence of sounds to a sequence of words.\n",
    "\n",
    "We start by choosing our vocabulary, the collection of symbols that we are going to be working with in each sequence. In our case, there will be two different sets of symbols, one for the input sequence to represent vocal sounds and one for the output sequence to represent words.\n",
    "\n",
    "For now, let's assume we're working with English. There are tens of thousands of words in the English language, and perhaps another few thousand to cover computer-specific terminology. That would give us a vocabulary size that is the better part of a hundred thousand. One way to convert words to numbers is to start counting at one and assign each word its own number. Then a sequence of words can be represented as a list of numbers.\n",
    "\n",
    "For example, consider a tiny language with a vocabulary size of three: files, find, and my. Each word could be swapped out for a number, perhaps files = 1, find = 2, and my = 3. Then the sentence \"Find my files\", consisting of the word sequence [ find, my, files ] could be represented instead as the sequence of numbers [2, 3, 1].\n",
    "\n",
    "This is a perfectly valid way to convert symbols to numbers, but it turns out that there's another format that's even easier for computers to work with, one-hot encoding. In one-hot encoding a symbol is represented by an array of mostly zeros, the same length of the vocabulary, with only a single element having a value of one. Each element in the array corresponds to a separate symbol.\n",
    "\n",
    "Another way to think about one-hot encoding is that each word still gets assigned its own number, but now that number is an index to an array. Here is our example above, in one-hot notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['files', 'find', 'my']\n"
     ]
    }
   ],
   "source": [
    "entire_corpus = \"Find my files\".lower()\n",
    "corpus = sorted(entire_corpus.split())\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://e2eml.school/images/transformers/one_hot_vocabulary.png)\n",
    "\n",
    "So the sentence \"Find my files\" becomes a sequence of one-dimensional arrays, which, after you squeeze them together, starts to look like a two-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['files', 'find', 'my']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0], [0, 1, 0], [0, 0, 1]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create function to return one-hot vector from a list of words\n",
    "def one_hot_vector(word_list, corpus):\n",
    "    one_hot_vector = []\n",
    "    for word in word_list:\n",
    "        one_hot_vector.append([1 if word == corpus[i] else 0 for i in range(len(corpus))])\n",
    "    return one_hot_vector\n",
    "print(corpus)\n",
    "one_hot_vector(corpus, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://e2eml.school/images/transformers/one_hot_sentence.png)\n",
    "\n",
    "Heads-up, I'll be using the terms \"one-dimensional array\" and \"vector\" interchangeably. Likewise with \"two-dimensional array\" and \"matrix\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0], [0, 0, 1], [1, 0, 0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_to_one_hot(word, corpus):\n",
    "    return one_hot_vector([word], corpus)[0]\n",
    "\n",
    "[word_to_one_hot(word, corpus) for word in [\"find\", \"my\", \"files\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot product\n",
    "One really useful thing about the one-hot representation is that it lets us compute dot products. These are also known by other intimidating names like inner product and scalar product. To get the dot product of two vectors, multiply their corresponding elements, then add the results.\n",
    "\n",
    "![](https://e2eml.school/images/transformers/dot_product.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_1 = np.array([0, 1, 1, 2])\n",
    "array_2 = np.array([1, 0, 1, 2])\n",
    "array_1.dot(array_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot products are especially useful when we're working with our one-hot word representations. The dot product of any one-hot vector with itself is one.\n",
    "\n",
    "![](https://e2eml.school/images/transformers/match.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_1 = np.array([0, 1, 0, 0])\n",
    "array_2 = np.array([0, 1, 0, 0])\n",
    "array_1.dot(array_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the dot product of any one-hot vector with any other one-hot vector is zero.\n",
    "\n",
    "![](https://e2eml.school/images/transformers/non_match.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_1 = np.array([0, 1, 0, 0])\n",
    "array_2 = np.array([0, 0, 0, 1])\n",
    "array_1.dot(array_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous two examples show how dot products can be used to measure similarity. As another example, consider a vector of values that represents a combination of words with varying weights. A one-hot encoded word can be compared against it with the dot product to show how strongly that word is represented.\n",
    "\n",
    "![](https://e2eml.school/images/transformers/similarity.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_1 = np.array([0, 0, 1, 0]) # Word\n",
    "array_2 = np.array([0.2, 0.7, 0.8, 0.1]) # Combination of words\n",
    "array_1.dot(array_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
